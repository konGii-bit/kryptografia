{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a47582d3",
      "metadata": {},
      "source": [
        "### Przykład predykcji przy użyciu szyfrowania homomorficznego i sieci neuronowej na zbiorze MNIST\n",
        "\n",
        "Schemat:\n",
        "\n",
        "- Uczymy model na danych niezaszyfrowanych\n",
        "- Robimy testową predykcję na danych niezaszyfrowanych\n",
        "- Używając wag modelu tworzymy predykcję na danych zaszyfrowanych"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f9468bc",
      "metadata": {},
      "source": [
        "Importujemy potrzebne biblioteki dla modelu jawnego"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fb68758",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "997250ae",
      "metadata": {},
      "source": [
        "Uczymy model jawny na danych niezaszyfrowanych. Z przyczyn sprzętowych ustawiamy test_batch_size na jeden. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a5c8580",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a5c8580",
        "outputId": "ff67f578-f22c-4aab-8897-d0cffcf1dcc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 12.8MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 344kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.23MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.11MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch shape: torch.Size([64, 1, 28, 28])\n",
            "Target shape: torch.Size([64])\n",
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
            "  (sigm): Sigmoid()\n",
            "  (fc2): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n",
            "Epoch [1/10], Loss: 2.1845\n",
            "Epoch [2/10], Loss: 1.7424\n",
            "Epoch [3/10], Loss: 1.2242\n",
            "Epoch [4/10], Loss: 0.9166\n",
            "Epoch [5/10], Loss: 0.7478\n",
            "Epoch [6/10], Loss: 0.6449\n",
            "Epoch [7/10], Loss: 0.5761\n",
            "Epoch [8/10], Loss: 0.5265\n",
            "Epoch [9/10], Loss: 0.4893\n",
            "Epoch [10/10], Loss: 0.4601\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(device)\n",
        "\n",
        "train_batch_size = 64\n",
        "test_batch_size = 1\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    root=\"./data\", train=True, transform=transform, download=True\n",
        ")\n",
        "test_dataset = torchvision.datasets.MNIST(\n",
        "    root=\"./data\", train=False, transform=transform, download=True\n",
        ")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset=train_dataset, batch_size=train_batch_size, shuffle=True\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset=test_dataset, batch_size=test_batch_size, shuffle=False\n",
        ")\n",
        "\n",
        "examples = iter(train_loader)\n",
        "example_data, example_targets = next(examples)\n",
        "\n",
        "print(f\"Batch shape: {example_data.shape}\")  # Should be [batch_size, channels, height, width]\n",
        "print(f\"Target shape: {example_targets.shape}\")  # Should be [batch_size]\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(28*28, 64)\n",
        "        self.sigm = nn.Sigmoid()\n",
        "        self.fc2 = nn.Linear(64,10)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.sigm(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = Net().to(device)\n",
        "print(model)\n",
        "\n",
        "learning_rate = 1e-2\n",
        "losses = []\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)           # Step 1: Get predictions\n",
        "        loss = criterion(outputs, labels) # Step 2: Measure error\n",
        "        # Backward pass and optimize\n",
        "        optimizer.zero_grad()             # Step 3: Clear old gradients\n",
        "        loss.backward()                   # Step 4: Compute new gradients\n",
        "        optimizer.step()                  # Step 5: Update model weights\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "    losses.append(running_loss/len(train_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2f6869f",
      "metadata": {},
      "source": [
        "Sprawdzamy dokładność modelu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "c0543ea0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0543ea0",
        "outputId": "dc3f5923-cb7d-4a30-bbaa-b2ed15186079"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "processing time for 100 images: 0.06319689750671387\n",
            "Accuracy on clean test images: 88.00% (expect ~90%)\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "start = time.time()\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        if total == 100: break\n",
        "\n",
        "print(f\"processing time for 100 images: {time.time() - start}\")\n",
        "\n",
        "clean_acc = 100 * correct / total\n",
        "print(f\"Accuracy on clean test images: {clean_acc:.2f}% (expect ~90%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RXnGu8fDDjUs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXnGu8fDDjUs",
        "outputId": "97886ade-a6d8-4799-e0f2-1b6576fa1df6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tenseal\n",
            "  Downloading tenseal-0.3.16-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Downloading tenseal-0.3.16-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (4.8 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/4.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/4.8 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenseal\n",
            "Successfully installed tenseal-0.3.16\n"
          ]
        }
      ],
      "source": [
        "!pip install tenseal"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48ee357c",
      "metadata": {},
      "source": [
        "Tworzymy kontekst i klucze galois. Następnie definiujemy fazę forward na wektorach zaszyfrowanych, przy użyciu wag i biasów poprzedniego modelu. Szyfrowanie CKKS nie pozwala na użycie funkcji sigmoid, więc używamy wielomianowego przybliżenia stopnia 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jgQLxX2X53je",
      "metadata": {
        "id": "jgQLxX2X53je"
      },
      "outputs": [],
      "source": [
        "import tenseal as ts\n",
        "\n",
        "context = ts.context(\n",
        "    ts.SCHEME_TYPE.CKKS,\n",
        "    poly_modulus_degree=8192,\n",
        "    coeff_mod_bit_sizes = [30, 20, 20, 20, 20, 30]\n",
        ")\n",
        "\n",
        "context.global_scale = 2**20\n",
        "\n",
        "context.generate_galois_keys()\n",
        "\n",
        "class EncryptedNet:\n",
        "    def __init__(self, torch_nn : Net):\n",
        "\n",
        "        self.fc1_weight = torch_nn.fc1.weight.T.data.tolist()\n",
        "        self.fc1_bias = torch_nn.fc1.bias.data.tolist()\n",
        "        self.fc2_weight = torch_nn.fc2.weight.T.data.tolist()\n",
        "        self.fc2_bias = torch_nn.fc2.bias.data.tolist()\n",
        "\n",
        "    def forward(self, enc_x):\n",
        "        enc_x = enc_x.mm(self.fc1_weight) + self.fc1_bias\n",
        "        enc_x = self.encrypted_sigmoid(enc_x)\n",
        "        enc_x = enc_x.mm(self.fc2_weight) + self.fc2_bias\n",
        "        return enc_x\n",
        "\n",
        "    def encrypted_sigmoid(self, enc_x):\n",
        "        return enc_x.polyval([0.5, 0.197, 0, -0.004])\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self.forward(*args, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2cea363",
      "metadata": {},
      "source": [
        "Tworzymy model zaszyfrowany"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ivt7Uipc56Xn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ivt7Uipc56Xn",
        "outputId": "2823c3fe-099e-4e73-b912-f781119202c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<__main__.EncryptedNet object at 0x7ae2c1a4c6d0>\n"
          ]
        }
      ],
      "source": [
        "enc_model = EncryptedNet(model)\n",
        "print(enc_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32ca46e4",
      "metadata": {},
      "source": [
        "Sprawdzamy dokładność i czas pracy modelu, który używa danych zaszyfowanych. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52d6d924",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52d6d924",
        "outputId": "97c1dfb7-4cab-4d25-c10c-7999393d4f7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 1 samples, Accuracy: 100.00%\n",
            "Processed 2 samples, Accuracy: 100.00%\n",
            "Processed 3 samples, Accuracy: 100.00%\n",
            "Processed 4 samples, Accuracy: 100.00%\n",
            "Processed 5 samples, Accuracy: 100.00%\n",
            "Processed 6 samples, Accuracy: 100.00%\n",
            "Processed 7 samples, Accuracy: 100.00%\n",
            "Processed 8 samples, Accuracy: 100.00%\n",
            "Processed 9 samples, Accuracy: 88.89%\n",
            "Processed 10 samples, Accuracy: 90.00%\n",
            "Processed 11 samples, Accuracy: 90.91%\n",
            "Processed 12 samples, Accuracy: 91.67%\n",
            "Processed 13 samples, Accuracy: 92.31%\n",
            "Processed 14 samples, Accuracy: 92.86%\n",
            "Processed 15 samples, Accuracy: 93.33%\n",
            "Processed 16 samples, Accuracy: 93.75%\n",
            "Processed 17 samples, Accuracy: 94.12%\n",
            "Processed 18 samples, Accuracy: 94.44%\n",
            "Processed 19 samples, Accuracy: 94.74%\n",
            "Processed 20 samples, Accuracy: 95.00%\n",
            "Processed 21 samples, Accuracy: 95.24%\n",
            "Processed 22 samples, Accuracy: 95.45%\n",
            "Processed 23 samples, Accuracy: 95.65%\n",
            "Processed 24 samples, Accuracy: 95.83%\n",
            "Processed 25 samples, Accuracy: 96.00%\n",
            "Processed 26 samples, Accuracy: 96.15%\n",
            "Processed 27 samples, Accuracy: 96.30%\n",
            "Processed 28 samples, Accuracy: 96.43%\n",
            "Processed 29 samples, Accuracy: 96.55%\n",
            "Processed 30 samples, Accuracy: 96.67%\n",
            "Processed 31 samples, Accuracy: 96.77%\n",
            "Processed 32 samples, Accuracy: 96.88%\n",
            "Processed 33 samples, Accuracy: 96.97%\n",
            "Processed 34 samples, Accuracy: 94.12%\n",
            "Processed 35 samples, Accuracy: 94.29%\n",
            "Processed 36 samples, Accuracy: 94.44%\n",
            "Processed 37 samples, Accuracy: 94.59%\n",
            "Processed 38 samples, Accuracy: 94.74%\n",
            "Processed 39 samples, Accuracy: 94.87%\n",
            "Processed 40 samples, Accuracy: 95.00%\n",
            "Processed 41 samples, Accuracy: 95.12%\n",
            "Processed 42 samples, Accuracy: 95.24%\n",
            "Processed 43 samples, Accuracy: 95.35%\n",
            "Processed 44 samples, Accuracy: 95.45%\n",
            "Processed 45 samples, Accuracy: 95.56%\n",
            "Processed 46 samples, Accuracy: 95.65%\n",
            "Processed 47 samples, Accuracy: 93.62%\n",
            "Processed 48 samples, Accuracy: 93.75%\n",
            "Processed 49 samples, Accuracy: 93.88%\n",
            "Processed 50 samples, Accuracy: 94.00%\n",
            "Processed 51 samples, Accuracy: 94.12%\n",
            "Processed 52 samples, Accuracy: 94.23%\n",
            "Processed 53 samples, Accuracy: 94.34%\n",
            "Processed 54 samples, Accuracy: 94.44%\n",
            "Processed 55 samples, Accuracy: 94.55%\n",
            "Processed 56 samples, Accuracy: 94.64%\n",
            "Processed 57 samples, Accuracy: 94.74%\n",
            "Processed 58 samples, Accuracy: 94.83%\n",
            "Processed 59 samples, Accuracy: 94.92%\n",
            "Processed 60 samples, Accuracy: 95.00%\n",
            "Processed 61 samples, Accuracy: 95.08%\n",
            "Processed 62 samples, Accuracy: 95.16%\n",
            "Processed 63 samples, Accuracy: 93.65%\n",
            "Processed 64 samples, Accuracy: 92.19%\n",
            "Processed 65 samples, Accuracy: 92.31%\n",
            "Processed 66 samples, Accuracy: 92.42%\n",
            "Processed 67 samples, Accuracy: 91.04%\n",
            "Processed 68 samples, Accuracy: 91.18%\n",
            "Processed 69 samples, Accuracy: 91.30%\n",
            "Processed 70 samples, Accuracy: 91.43%\n",
            "Processed 71 samples, Accuracy: 91.55%\n",
            "Processed 72 samples, Accuracy: 91.67%\n",
            "Processed 73 samples, Accuracy: 91.78%\n",
            "Processed 74 samples, Accuracy: 90.54%\n",
            "Processed 75 samples, Accuracy: 90.67%\n",
            "Processed 76 samples, Accuracy: 90.79%\n",
            "Processed 77 samples, Accuracy: 90.91%\n",
            "Processed 78 samples, Accuracy: 89.74%\n",
            "Processed 79 samples, Accuracy: 89.87%\n",
            "Processed 80 samples, Accuracy: 90.00%\n",
            "Processed 81 samples, Accuracy: 88.89%\n",
            "Processed 82 samples, Accuracy: 89.02%\n",
            "Processed 83 samples, Accuracy: 89.16%\n",
            "Processed 84 samples, Accuracy: 89.29%\n",
            "Processed 85 samples, Accuracy: 89.41%\n",
            "Processed 86 samples, Accuracy: 89.53%\n",
            "Processed 87 samples, Accuracy: 89.66%\n",
            "Processed 88 samples, Accuracy: 88.64%\n",
            "Processed 89 samples, Accuracy: 88.76%\n",
            "Processed 90 samples, Accuracy: 88.89%\n",
            "Processed 91 samples, Accuracy: 89.01%\n",
            "Processed 92 samples, Accuracy: 89.13%\n",
            "Processed 93 samples, Accuracy: 88.17%\n",
            "Processed 94 samples, Accuracy: 88.30%\n",
            "Processed 95 samples, Accuracy: 88.42%\n",
            "Processed 96 samples, Accuracy: 88.54%\n",
            "Processed 97 samples, Accuracy: 88.66%\n",
            "Processed 98 samples, Accuracy: 87.76%\n",
            "Processed 99 samples, Accuracy: 87.88%\n",
            "Processed 100 samples, Accuracy: 88.00%\n",
            "processing time for 100 images: 2527.509290933609\n",
            "Accuracy on test images: 88.00%\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in test_loader:\n",
        "\n",
        "        image = images.view(-1, 28*28).numpy().flatten()\n",
        "\n",
        "        x_enc = ts.ckks_vector(context, image)\n",
        "        \n",
        "        # klasyfikujemy na danych zaszyfrowanych\n",
        "        enc_output = enc_model(x_enc)\n",
        "        \n",
        "        # deszyfrujemy wynik\n",
        "        decrypted_output = enc_output.decrypt()\n",
        "        output = torch.tensor(decrypted_output, dtype=torch.float32).view(1, -1)\n",
        "        \n",
        "        # obliczamy dokładność\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total += 1\n",
        "        correct += (predicted.item() == labels.item())\n",
        "\n",
        "        print(f\"Processed {total} samples, Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "        if total == 100: break\n",
        "\n",
        "    print(f\"processing time for 100 images: {time.time() - start}\")\n",
        "\n",
        "    print(f'Accuracy on test images: {100 * correct / total:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b65ab36",
      "metadata": {},
      "source": [
        "Wnioski: Z uwagi na prostotę danych udało się uzyskać identyczną dokładność (88%), okupione zostało to o wiele większym czasem wykonania (ok. x4200). Użycie przybliżenia funkcji sigmoid, i fakt że pojedyńczy obraz ma 728px ma spory wpływ na czas wykonania."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "crypto_proj",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
